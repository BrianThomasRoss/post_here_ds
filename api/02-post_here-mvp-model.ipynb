{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Here: Subreddit Predictor\n",
    "\n",
    "## Recommendation API - 1.1\n",
    "\n",
    "> aka: the MVP Classifier®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Intro - MVP Classifier (Model #2)\n",
    "\n",
    "The second iteration of the model for recommending (predicting) appropriate subreddit(s) will be built using a somewhat naive approach to text analysis and a multi-class classification model.\n",
    "\n",
    "The model will be trained using the [reddit self-post classification task dataset](https://www.kaggle.com/mswarbrickjones/reddit-selfposts), available on Kaggle thanks to [Evolution AI](https://evolution.ai//blog/page/5/an-imagenet-like-text-classification-task-based-on-reddit-posts/).\n",
    "\n",
    "The full dataset includes 1,013,000 rows (1000 records each from 1013 subreddits). To keep things manageable for the proof-of-concept, the first 100,000 records will be used to train the embeddings and models.\n",
    "\n",
    "According to the dataset description on Kaggle, the data has already been randomized. Therefore, reading in the first 100k records, as opposed to extracting a random sample, will not introduce any more bias into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General imports === #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === sklearn imports === #\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             subreddit  \\\n",
       "0   0  talesfromtechsupport   \n",
       "1   1               teenmom   \n",
       "2   2                Harley   \n",
       "3   3          ringdoorbell   \n",
       "4   4                 intel   \n",
       "\n",
       "                                               title  \\\n",
       "0             Remember your command line switches...   \n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                            selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  \n",
       "1  Did he ever say what his addiction was or is h...  \n",
       "2  Funny story. I went to college in Las Vegas. T...  \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...  \n",
       "4  Prime95 (regardless of version) and OCCT both,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load the dataset === #\n",
    "df1 = pd.read_csv(\"rspct_100k.csv\", sep=\"\\t\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             subreddit  \\\n",
       "0   0  talesfromtechsupport   \n",
       "1   1               teenmom   \n",
       "2   2                Harley   \n",
       "3   3          ringdoorbell   \n",
       "4   4                 intel   \n",
       "\n",
       "                                               title  \\\n",
       "0             Remember your command line switches...   \n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                            selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  \n",
       "1  Did he ever say what his addiction was or is h...  \n",
       "2  Funny story. I went to college in Las Vegas. T...  \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...  \n",
       "4  Prime95 (regardless of version) and OCCT both,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === First looks === #\n",
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MVP model will be trained on the selftext column only. Thus, we only need `selftext` (X) and `subreddit` (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 4), (20000, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Split up dataset into train and test === #\n",
    "\n",
    "# 80% train, 20% test, stratified on the target\n",
    "train, test = train_test_split(df1, test_size=0.2, stratify=df1[\"subreddit\"])\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Arrange data into feature and target === #\n",
    "\n",
    "X_train = train[\"selftext\"]\n",
    "X_test = test[\"selftext\"]\n",
    "\n",
    "y_train = train[\"subreddit\"]\n",
    "y_test = test[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000,) (20000,)\n",
      "(80000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The vectorizer that will be used to convert the words into numbers will not analyze the text for meaning or anything like that - remember, this is the MVP model. We can get as crazy as we want after we have a working baseline.\n",
    "\n",
    "TF-IDF vectorization finds the unique aspects of documents of text, based on a simple count of the words within each document. In this context, \"document\" refers to an individual reddit post.\n",
    "\n",
    "The vectorizer can be instantiated then \"trained\" on the dataset. One way to think about the training step is that it builds a vectorized vocabulary of the words in the dataset.\n",
    "\n",
    "The TF-IDF implementation that will be used in the MVP comes from scikit-learn:\n",
    "\n",
    "> [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "A custom tokenizer function can be passed into the vectoirizer to increase the quality of the tokens (number representations of words). The tokenizer function will use the NLP library [spaCy](https://spacy.io/usage/).\n",
    "\n",
    "However, for the MVP, we will go with the built-in tokenizer function built into the TfidfVectorizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# === Custom tokenizer function === #\n",
    "\n",
    "def tokenize(doc):\n",
    "    \"\"\"\n",
    "    Extracts nouns and adjectives from a string of text.\n",
    "    Returns a list of spacy token.lemma objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(doc)\n",
    "    na_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (\n",
    "            ((token.is_stop == False) and (token.is_punct == False))\n",
    "            and (token.pos_ == \"NOUN\")\n",
    "            or (token.pos_ == \"ADJ\")\n",
    "        ):\n",
    "            na_tokens.append(token.lemma_.strip().lower())\n",
    "\n",
    "    return na_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([650, 246, 191, 215, 426, 779, 400, 502])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Encode the target using LabelEncoder === #\n",
    "\n",
    "# This process naively transforms each category of the target into a number\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder() # Instantiate a new encoder instance\n",
    "le.fit(y_train)  # Fit it on training label data\n",
    "\n",
    "# Transform both using the train fit\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "y_train[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "27ddcca6c2af541e94133222961a83c11f54208e"
   },
   "outputs": [],
   "source": [
    "# === Vectorize! === #\n",
    "\n",
    "# Extract features from the text data using the bag-of-words approach (single words + bigrams).\n",
    "# Uses tfidf weighting (helps a little for Naive Bayes in general).\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# TODO: use spacy's stopwords\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100000,\n",
    "    min_df=5,\n",
    "    ngram_range=(1,2),\n",
    "    stop_words=None,\n",
    "    token_pattern='(?u)\\\\b\\\\w+\\\\b',\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the feature column to create vocab\n",
    "# This process is split into component parts to make pickling the vocab simpler\n",
    "vocab = tfidf.fit(X_train)\n",
    "\n",
    "# Get sparse document-term matrix for training data\n",
    "X_train_sparse = tfidf.transform(X_train)\n",
    "\n",
    "# Get sparse document-term matrix for test data\n",
    "X_test_sparse = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "27ddcca6c2af541e94133222961a83c11f54208e",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810400, 50000), (202600, 50000))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Prune the resulting featureset to most useful features === #\n",
    "\n",
    "# Instantiate the selector\n",
    "chi2_selector = SelectKBest(chi2, 50000)\n",
    "\n",
    "# Fit on the training data\n",
    "chi2_selector.fit(X_train_sparse, y_train) \n",
    "\n",
    "# Transform both train and test feature\n",
    "X_train_select = chi2_selector.transform(X_train_sparse)\n",
    "X_test_select  = chi2_selector.transform(X_test_sparse)\n",
    "\n",
    "# Take a look at the result\n",
    "X_train_select.shape, X_test_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Naive Bayes model === #\n",
    "\n",
    "# Instantiate and train the model\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "nb.fit(X_train_sparse, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1013)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00025731, 0.0017611 , 0.0004326 , ..., 0.00104355, 0.00089922,\n",
       "        0.00120952],\n",
       "       [0.00024139, 0.00145366, 0.00139822, ..., 0.00128417, 0.0007256 ,\n",
       "        0.00069661],\n",
       "       [0.00055992, 0.00337738, 0.00059392, ..., 0.00074112, 0.00057163,\n",
       "        0.00061933],\n",
       "       ...,\n",
       "       [0.00030182, 0.00194612, 0.00069209, ..., 0.00124686, 0.00065143,\n",
       "        0.00065566],\n",
       "       [0.00034963, 0.00143321, 0.00102417, ..., 0.00224432, 0.00052771,\n",
       "        0.0008239 ],\n",
       "       [0.00048969, 0.00347422, 0.00072534, ..., 0.00075197, 0.00061453,\n",
       "        0.0021289 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Create predictions on test feature === #\n",
    "y_pred_proba = nb.predict_proba(X_test_sparse)\n",
    "\n",
    "print(y_pred_proba.shape)\n",
    "y_pred_proba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([862, 345, 654, 453, 867, 530, 453, 835, 126, 311])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === For each prediction, find the index with the highest probability === #\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "8d31738daa5d0382761477f16e79d1800ac6f730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.45935\n",
      "precision@3 = 0.6075\n",
      "precision@5 = 0.665\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate performance using precision-at-k === #\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argsort(y_pred, axis=1)\n",
    "    y_pred = y_pred[:, ::-1][:, :k]\n",
    "    arr = [y in s for y, s in zip(y_true, y_pred)]\n",
    "    return np.mean(arr)\n",
    "\n",
    "print('precision@1 =', np.mean(y_test == y_pred))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba, 5))\n",
    "\n",
    "# With all 1m records\n",
    "# precision@1 = 0.6732724580454097\n",
    "# precision@3 = 0.8058588351431392\n",
    "# precision@5 = 0.8481391905231984\n",
    "\n",
    "# With 100k records\n",
    "# precision@1 = 0.45935\n",
    "# precision@3 = 0.6075\n",
    "# precision@5 = 0.665"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Predict subreddit from new input\n",
    "\n",
    "Now that our model is trained and we have our baseline performance metric that is not half-bad, we can use the trained model to predict what subreddit would belong to a new piece of data (a post).\n",
    "\n",
    "In order to do this, the post will have to be vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Example post === #\n",
    "\n",
    "# The example comes from 'r/learnprogramming'\n",
    "post = \"\"\"I am a new grad looking for a job and currently in the process with a company for a junior backend engineer role. I was under the impression that the position was Javascript but instead it is actually Java. My general programming and \"leet code\" skills are pretty good, but my understanding of Java is pretty shallow. How can I use the next three days to best improve my general Java knowledge? Most resources on the web seem to be targeting complete beginners. Maybe a book I can skim through in the next few days?\n",
    "\n",
    "Edit:\n",
    "\n",
    "A lot of people are saying \"the company is a sinking ship don't even go to the interview\". I just want to add that the position was always for a \"junior backend engineer\". This company uses multiple languages and the recruiter just told me the incorrect language for the specific team I'm interviewing for. I'm sure they're mainly interested in seeing my understanding of good backend principles and software design, it's not a senior lead Java position.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Vectorize the example post using the trained vocab === #\n",
    "post_sparse = tfidf.transform([post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 178 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Transform using chi2 === #\n",
    "post_select = chi2_selector.transform(post_sparse)\n",
    "post_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00021132, 0.00085588, 0.00038891, ..., 0.00134066, 0.00053571,\n",
       "        0.00043089]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Generate prediction === #\n",
    "post_pred_proba = nb.predict_proba(post_sparse)\n",
    "post_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([197, 856, 407, 577, 400, 247, 874, 662, 993,  93])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Return the indices that would sort and return top 10 === #\n",
    "post_pred_proba_sorted = np.argsort(post_pred_proba[0])\n",
    "pred_sorted_index = post_pred_proba_sorted[:10]\n",
    "pred_sorted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KeybaseProofs' 'sharditkeepit' 'WouldYouRather' 'emojipasta'\n",
      " 'WayfarersPub' 'MtvChallenge' 'soccer' 'ifttt' 'whatsthisplant'\n",
      " 'DanLeBatardShow']\n"
     ]
    }
   ],
   "source": [
    "# === Plug that into the label classes to get hydrated af === #\n",
    "post_pred_top10 = le.classes_[pred_sorted_index]\n",
    "print(post_pred_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_pred_top10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test it out with another dummy post === #\n",
    "\n",
    "# This one comes from r/suggestmeabook\n",
    "post2 = \"\"\"I've been dreaming about writing my own stort story for a while but I want to give it an unexpected ending. I've read lots of books, but none of them had the plot twist I want. I want to read books with the best plot twists, so that I can analyze what makes a good plot twist and write my own story based on that points. I don't like romance novels and I mostly enjoy sci-fi or historical books but anything beside romance novels would work for me, it doesn't have to be my type of novel. I'm open to experience after all. I need your help guys. Thanks in advance.\"\"\"\n",
    "\n",
    "# === Vectorize the example post using the trained vocab === #\n",
    "post2_sparse = tfidf.transform([post2])\n",
    "\n",
    "# === Generate prediction === #\n",
    "post2_pred_proba = nb.predict_proba(post2_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Return the indices that would sort and return top 10 === #\n",
    "pred2_sorted_index = np.argsort(post2_pred_proba[0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KeybaseProofs' 'sharditkeepit' 'WouldYouRather' 'emojipasta'\n",
      " 'WayfarersPub' 'MtvChallenge' 'soccer' 'ifttt' 'whatsthisplant'\n",
      " 'DanLeBatardShow']\n"
     ]
    }
   ],
   "source": [
    "# === Plug that into the label classes to get hydrated af === #\n",
    "post2_pred_top10 = le.classes_[pred2_sorted_index]\n",
    "print(post_pred_top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are the same, meaning something didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1013)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post2_pred_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>0.156121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>0.029756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>0.027238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.010433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.008873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1013 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         proba\n",
       "905   0.156121\n",
       "579   0.029756\n",
       "1005  0.027238\n",
       "74    0.010433\n",
       "101   0.008873\n",
       "...        ...\n",
       "874   0.000056\n",
       "247   0.000053\n",
       "400   0.000037\n",
       "856   0.000028\n",
       "197   0.000002\n",
       "\n",
       "[1013 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post2_pred = pd.DataFrame(post2_pred_proba.T, columns=[\"proba\"])\n",
    "df_post2_pred.sort_values(by=\"proba\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sort the resulting array and return top 10 === #\n",
    "post_pred_proba_sorted = np.sort(post_pred_proba, axis=None)\n",
    "post_pred_proba_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (1, 1013)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-6bfa377257a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Inverse transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpost_pred_proba_inversed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_pred_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.vega/neural_draft-sDaGcGcX/lib/python3.7/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# inverse transform of empty array is empty array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vega/neural_draft-sDaGcGcX/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape (1, 1013)"
     ]
    }
   ],
   "source": [
    "# Inverse transform\n",
    "post_pred_proba_inversed = le.inverse_transform(post_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.592764e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.022920e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.042813e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.616618e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.187258e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index             0\n",
       "0      0  6.592764e-07\n",
       "1      1  1.022920e-05\n",
       "2      2  1.042813e-06\n",
       "3      3  1.616618e-06\n",
       "4      4  4.187258e-06"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cast result to dataframe === #\n",
    "df_ppp = pd.DataFrame(post_pred_proba.T).reset_indexdex()\n",
    "df_ppp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810400,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Look at the encoded labels to be sure it matches === #\n",
    "le.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Now to inverse transform the encoded labels === #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0  691\n",
       "1  289\n",
       "2  117\n",
       "3  952\n",
       "4   77"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_df = pd.DataFrame(y_train)\n",
    "y_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['13ReasonsWhy', '3Dprinting', '3d6', ..., 'yoga', 'yorku',\n",
       "       'zootopia'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Picklization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create pickle func to make pickling (a little) easier === #\n",
    "\n",
    "def picklizer(to_pickle, filename, path):\n",
    "    \"\"\"\n",
    "    Creates a pickle file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    to_pickle : Python object\n",
    "        The trained / fitted instance of the \n",
    "        transformer or model to be pickled.\n",
    "    filename : string\n",
    "        The desired name of the output file,\n",
    "        not including the '.pkl' extension.\n",
    "    path : string or path-like object\n",
    "        The path to the desired output directory.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    # Create the path to save location\n",
    "    picklepath = os.path.join(path, filename)\n",
    "\n",
    "    # Use context manager to open file\n",
    "    with open(picklepath, \"wb\") as p:\n",
    "        pickle.dump(to_pickle, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Picklize! === #\n",
    "filepath = \"../../models\"\n",
    "\n",
    "# Export vectorizer as pickle\n",
    "picklizer(vocab, \"02_vocab.pkl\", filepath)\n",
    "\n",
    "# Export chi2 selector as pickle\n",
    "picklizer(chi2_selector, \"02_chi.pkl\", filepath)\n",
    "\n",
    "# Export naive bayes model as pickle\n",
    "picklizer(nb, \"02_nb.pkl\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
